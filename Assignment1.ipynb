{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "using_corpus = gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import re\n",
    "\n",
    "word_regex = re.compile('[a-zA-Z]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\applicationarea\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3495b4b33d16496686ec9536539e4069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98552.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "adverbs = {}\n",
    "predicates = {}\n",
    "adv_any_bigrams = {}\n",
    "adv_pred_bigrams = {}\n",
    "\n",
    "sentences_portion = 1\n",
    "sentences = using_corpus.sents()\n",
    "sentences = sentences[:int(len(sentences) * sentences_portion)]\n",
    "sentences = [\n",
    "    [ word.lower() for word in words if word_regex.match(word) ] for words in sentences\n",
    "]\n",
    "\n",
    "\n",
    "# 1. Preprocessing dataset\n",
    "# =====\n",
    "# Parse sentences into adverbs, predicates, adv_any_bigrams, adv_pred_bigrams\n",
    "# Which is {(word or bigram) => frequency}\n",
    "#     adverbs: Adverbs which are coming together with predicates in (adverb + predicate) form\n",
    "#     predicates: Predicates wich are coming together with adverbs in (adverb + predicate) form\n",
    "#     adv_any_bigrams: Bigrams which first element is an adverb\n",
    "#     adv_pred_bigrams: Bigrams which first element is an adverb and second element is a predicate\n",
    "\n",
    "for sent in tqdm(sentences):\n",
    "    tokens = nltk.pos_tag(sent)\n",
    "    for index, (word, pos) in enumerate(tokens):\n",
    "        if pos not in ('RB', 'RBR', 'RBS'):\n",
    "            continue\n",
    "        \n",
    "        if len(tokens) <= index + 1:\n",
    "            continue\n",
    "        \n",
    "        next_word, next_pos = tokens[index + 1]\n",
    "        \n",
    "        bigram = (word, next_word)\n",
    "        if bigram not in adv_any_bigrams:\n",
    "            adv_any_bigrams[bigram] = 0\n",
    "        \n",
    "        adv_any_bigrams[bigram] += 1\n",
    "        \n",
    "        if next_pos not in ('VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS'):\n",
    "            continue\n",
    "        \n",
    "        if word not in adverbs:\n",
    "            adverbs[word] = 0\n",
    "        \n",
    "        adverbs[word] += 1\n",
    "        \n",
    "        if next_word not in predicates:\n",
    "            predicates[next_word] = 0\n",
    "        \n",
    "        predicates[next_word] += 1\n",
    "        \n",
    "        if bigram not in adv_pred_bigrams:\n",
    "            adv_pred_bigrams[bigram] = 0\n",
    "        \n",
    "        adv_pred_bigrams[bigram] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "adv_bi_sim_memoization = {}\n",
    "def adverb_bigram_similarity(adv1, adv2):\n",
    "    # Getting cosine similarity between next word frequency vector of adverbs\n",
    "    \n",
    "    if adv1 == adv2:\n",
    "        return 1\n",
    "    \n",
    "    if (adv1, adv2) in adv_bi_sim_memoization:\n",
    "        return adv_bi_sim_memoization[(adv1, adv2)]\n",
    "    \n",
    "    adv1_words = {}\n",
    "    adv2_words = {}\n",
    "    \n",
    "    for bigram in adv_any_bigrams:\n",
    "        adv, next_word = bigram\n",
    "        \n",
    "        if adv != adv1 and adv != adv2:\n",
    "            continue\n",
    "\n",
    "        if next_word not in adv1_words:\n",
    "            adv1_words[next_word] = 0\n",
    "\n",
    "        if next_word not in adv2_words:\n",
    "            adv2_words[next_word] = 0\n",
    "\n",
    "        if adv == adv1:\n",
    "            adv1_words[next_word] += 1\n",
    "\n",
    "        else:\n",
    "            adv2_words[next_word] += 1\n",
    "    \n",
    "    adv1_value_vec = np.asarray([\n",
    "        freq for word, freq in sorted(\n",
    "            adv1_words.items(),\n",
    "            key = lambda x: x[0]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    adv2_value_vec = np.asarray([\n",
    "        freq for word, freq in sorted(\n",
    "            adv2_words.items(),\n",
    "            key = lambda x: x[0]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    similarity = np.dot(adv1_value_vec, adv2_value_vec) / \\\n",
    "        (np.linalg.norm(adv1_value_vec) * np.linalg.norm(adv2_value_vec))\n",
    "    \n",
    "    adv_bi_sim_memoization[(adv1, adv2)] = similarity\n",
    "    \n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 ['very', 'most', 'highly', 'too', 'however', 'so', 'well', 'almost', 'really', 'yet', 'ever', 'particularly', 'more', 'quite', 'certainly', 'therefore', 'perfectly', 'even', 'rather', 'then', 'truly', 'often', 'still', 'there', 'just', 'away', 'totally', 'pretty', 'better', 'altogether', 'remarkably', 'less', 'here', 'again', 'exceedingly', 'down', 'thus', 'once', 'equally', 'extremely', 'together', 'somewhat', 'o', 'ere', 'jolly']\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import vader\n",
    "\n",
    "bigrams = {}\n",
    "intensifiers = {}\n",
    "not_intensifiers = ['that', 'back', 'only', 'always', 'now', 'as', 'also']\n",
    "\n",
    "variance_threshold = 10.0\n",
    "similarity_threshold = 0.05\n",
    "adverb_variance = {}\n",
    "adverb_mean = {}\n",
    "adverb_count = {}\n",
    "\n",
    "# 2. Finding intensifier\n",
    "# =====\n",
    "# Find adverb which\n",
    "#   * is not NEGATE (not, never, ....) and\n",
    "#   * is not (only, always, now, as) and\n",
    "#   * does not appear with certain predicates (this is why the threshold is low enough) and\n",
    "#   * has similar next word frequency with 'very'\n",
    "#\n",
    "# But actually, the final one does not worked well, so I have lowered the threshold\n",
    "\n",
    "for bigram in adv_pred_bigrams:\n",
    "    if bigram[0] not in adverb_count:\n",
    "        adverb_count[bigram[0]] = 0\n",
    "    \n",
    "    adverb_count[bigram[0]] += 1\n",
    "\n",
    "for adverb, predicate_count in adverb_count.items():\n",
    "    adverb_mean[adverb] = adverbs[adverb] / predicate_count\n",
    "\n",
    "for bigram, frequency in adv_pred_bigrams.items():\n",
    "    adverb, predicate = bigram\n",
    "    if adverb not in adverb_variance:\n",
    "        adverb_variance[adverb] = 0\n",
    "    \n",
    "    adverb_variance[adverb] += (frequency - adverb_mean[adverb]) ** 2\n",
    "\n",
    "for adverb, variance in adverb_variance.items():\n",
    "    if not (\n",
    "        adverb in vader.NEGATE or\n",
    "        adverb in not_intensifiers or\n",
    "        (variance <= variance_threshold) or\n",
    "        adverb_bigram_similarity('very', adverb) < similarity_threshold\n",
    "    ): # or (adverb in vader.BOOSTER_DICT):\n",
    "        \n",
    "        if adverb not in intensifiers:\n",
    "            intensifiers[adverb] = 0\n",
    "        \n",
    "        intensifiers[adverb] += frequency\n",
    "        bigrams[bigram] = frequency\n",
    "\n",
    "print(len(intensifiers), list(intensifiers.keys())[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(word):\n",
    "    synsets = wn.synsets(word)\n",
    "    if len(synsets) < 1:\n",
    "        return []\n",
    "\n",
    "    synonyms = set()\n",
    "    for synset in synsets:\n",
    "        if synset.pos() == 'n':\n",
    "            continue\n",
    "            \n",
    "        synonyms.update(synset.lemma_names())\n",
    "\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_similar = {}\n",
    "\n",
    "# 3. Finding similar words for predicates\n",
    "# =====\n",
    "# Find similar words by exploring lemma_names in wordnet\n",
    "\n",
    "for predicate in predicates:\n",
    "    similars = find_similar(predicate)\n",
    "    similar_predicates = [\n",
    "        similar for similar in similars\n",
    "        if similar in predicates and similar != predicate\n",
    "    ]\n",
    "    \n",
    "    if len(similar_predicates) > 0:\n",
    "        predicate_similar[predicate] = similar_predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2b9bc5ca354ff9a9f0be51a7a40315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7618.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "results = []\n",
    "\n",
    "# 4. Finding triples\n",
    "# =====\n",
    "# The triples are found from wordnet definition.\n",
    "# The details are like this:\n",
    "#\n",
    "# 1) For each predicates, find similar words\n",
    "# 2) For each words, find synsets from wordnet\n",
    "# 3) For each synset, get definition and tokenize\n",
    "# 4) Find that the definition contains any intensifiers\n",
    "# 5) If it has an intensifier, add to result set\n",
    "\n",
    "for predicate in tqdm(predicates):\n",
    "    if predicate not in predicate_similar:\n",
    "        continue\n",
    "    \n",
    "    similar_words = predicate_similar[predicate]\n",
    "    \n",
    "    for similar_word in similar_words:\n",
    "        synsets = wn.synsets(similar_word)\n",
    "        containing_intensifiers = []\n",
    "        \n",
    "        for synset in synsets:\n",
    "            definition = synset.definition()\n",
    "            tokens = word_tokenize(definition)\n",
    "\n",
    "            if predicate not in tokens:\n",
    "                continue\n",
    "            \n",
    "            token_len = len(tokens)\n",
    "            \n",
    "            for index, token in enumerate(tokens):\n",
    "                if token not in intensifiers:\n",
    "                    continue\n",
    "                \n",
    "                containing_intensifiers.append(token)\n",
    "                break\n",
    "        \n",
    "        highest_intensifier = (0, None)\n",
    "        for intensifier in containing_intensifiers:\n",
    "            very_similarity = adverb_bigram_similarity('very', intensifier)\n",
    "            if very_similarity > highest_intensifier[0]:\n",
    "                highest_intensifier = (very_similarity, intensifier)\n",
    "        \n",
    "        if highest_intensifier[1] == None:\n",
    "            continue\n",
    "        \n",
    "        results.append(\n",
    "            (predicate, intensifier, similar_word)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pass', 'down', 'return'), ('progress', 'better', 'advance'), ('awake', 'there', 'wake'), ('poor', 'very', 'miserable'), ('spring', 'away', 'bound'), ('close', 'so', 'tight'), ('restore', 'together', 'repair'), ('require', 'just', 'ask'), ('humble', 'even', 'modest'), ('whole', 'together', 'solid'), ('drop', 'less', 'fell'), ('regarded', 'well', 'regard'), ('esteemed', 'well', 'respect'), ('sink', 'down', 'subside'), ('right', 'most', 'good'), ('down', 'down', 'depressed'), ('go', 'away', 'depart'), ('pressing', 'more', 'compress'), ('running', 'then', 'draw'), ('maintain', 'often', 'hold'), ('calm', 'still', 'tranquillize'), ('force', 'away', 'push'), ('sink', 'down', 'settle'), ('restore', 'together', 'fix'), ('improbable', 'too', 'marvellous'), ('exhausted', 'extremely', 'fatigued'), ('work', 'rather', 'play'), ('expects', 'so', 'bear'), ('require', 'just', 'demand'), ('ascended', 'better', 'rise'), ('soften', 'less', 'moderate'), ('turning', 'away', 'turn'), ('disappear', 'less', 'melt'), ('miserable', 'most', 'abject'), ('pass', 'then', 'draw'), ('lasting', 'very', 'durable'), ('better', 'better', 'right'), ('control', 'often', 'hold'), ('extend', 'more', 'expand'), ('down', 'down', 'kill'), ('spring', 'away', 'recoil'), ('respected', 'well', 'respect'), ('restore', 'together', 'doctor'), ('disdain', 'down', 'despise'), ('lower', 'often', 'low'), ('lower', 'less', 'down'), ('close', 'very', 'near'), ('pressing', 'more', 'compact'), ('secret', 'so', 'confidential'), ('ascended', 'better', 'ascend')]\n"
     ]
    }
   ],
   "source": [
    "# 5. Saving it with CSV\n",
    "# =====\n",
    "# Save the found triples in a csv file\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('results.csv', 'w', encoding='utf-8', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    \n",
    "    for result in results[:50]:\n",
    "        writer.writerow(result)\n",
    "\n",
    "import random\n",
    "print(sorted(results, key = lambda x: random.random())[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nint_freq_by_pred = {}\\n\\nfor bigram, frequency in bigrams.items():\\n    intensifier, predicate = bigram\\n    if predicate not in int_freq_by_pred:\\n        int_freq_by_pred[predicate] = {}\\n    \\n    int_freq_by_pred[predicate][intensifier] = frequency\\n\\nfor predicate, int_freq in int_freq_by_pred.items():\\n    int_freq_by_pred[predicate] = sorted(\\n        int_freq.items(),\\n        key = lambda x: x[1]\\n    )\\n\\nfrequency_threshold = 20\\nresult = []\\nfor pred, similar_preds in predicate_similar.items():\\n    for similar_pred in similar_preds:\\n        if similar_pred not in int_freq_by_pred:\\n            continue\\n        \\n        if frequency_threshold < int_freq_by_pred[similar_pred][0][1]:\\n            continue\\n        \\n        result.append(\\n            (pred, int_freq_by_pred[similar_pred][0][0], similar_pred)\\n        )\\n\\nprint(result[:50])\\nlen(result)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Appendix 1. Old approach\n",
    "# =====\n",
    "# Finding an intensifier which is most coming with certain similar word of predicate\n",
    "\n",
    "\"\"\"\n",
    "words = set([ word for word in using_corpus.words() if word_regex.match(word) ])\n",
    "print(len(words))\n",
    "\n",
    "int_freq_by_pred = {}\n",
    "\n",
    "for bigram, frequency in bigrams.items():\n",
    "    intensifier, predicate = bigram\n",
    "    if predicate not in int_freq_by_pred:\n",
    "        int_freq_by_pred[predicate] = {}\n",
    "    \n",
    "    int_freq_by_pred[predicate][intensifier] = frequency\n",
    "\n",
    "for predicate, int_freq in int_freq_by_pred.items():\n",
    "    int_freq_by_pred[predicate] = sorted(\n",
    "        int_freq.items(),\n",
    "        key = lambda x: x[1]\n",
    "    )\n",
    "\n",
    "frequency_threshold = 20\n",
    "result = []\n",
    "for pred, similar_preds in predicate_similar.items():\n",
    "    for similar_pred in similar_preds:\n",
    "        if similar_pred not in int_freq_by_pred:\n",
    "            continue\n",
    "        \n",
    "        if frequency_threshold < int_freq_by_pred[similar_pred][0][1]:\n",
    "            continue\n",
    "        \n",
    "        result.append(\n",
    "            (pred, int_freq_by_pred[similar_pred][0][0], similar_pred)\n",
    "        )\n",
    "\n",
    "print(result[:50])\n",
    "len(result)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom gensim.models import word2vec\\n\\nmodel = word2vec.Word2Vec(\\n    sentences, \\n    workers=4,\\n    size=50,\\n    min_count=10,\\n    window=10,\\n    sample=1e-3\\n)\\n\\nbigrams = {}\\nintensifiers = {}\\n\\ndistance_threshold = 0.3\\ngeneral_frequency_threshold = 20.0\\n\\n### Filtering by very-similarity ###\\nfor bigram, frequency in adv_pred_bigrams.items():\\n    adverb, predicate = bigram\\n    \\n    if adverb not in model.wv:\\n        continue\\n    \\n    if adverbs[adverb] / frequency <= general_frequency_threshold:\\n        continue\\n    \\n    if model.wv.distance(adverb, 'very') > distance_threshold:\\n        continue\\n        \\n    if adverb not in intensifiers:\\n        intensifiers[adverb] = 0\\n        \\n    intensifiers[adverb] += frequency\\n    bigrams[bigram] = frequency\\n\\nprint(len(intensifiers), list(intensifiers.keys())[:50])\\n\\npredicate_similar = {}\\n\\nfor predicate in predicates:\\n    similars = find_similar(predicate)\\n    similar_predicates = [\\n        similar for similar in similars\\n        if similar in predicates and similar != predicate\\n    ]\\n    \\n    if len(similar_predicates) > 0:\\n        predicate_similar[predicate] = similar_predicates\\n\\nprint(predicate_similar)\\n\\nint_freq_by_pred = {}\\n\\nfor bigram, frequency in bigrams.items():\\n    intensifier, predicate = bigram\\n    if predicate not in int_freq_by_pred:\\n        int_freq_by_pred[predicate] = {}\\n    \\n    int_freq_by_pred[predicate][intensifier] = frequency\\n\\nfor predicate, int_freq in int_freq_by_pred.items():\\n    int_freq_by_pred[predicate] = sorted(\\n        int_freq.items(),\\n        key = lambda x: x[1]\\n    )\\n\\n\\nfrom numpy import dot\\nfrom numpy.linalg import norm\\n\\npred_similarity_threshold = 0.7\\nfrequency_threshold = 20\\nresult = []\\n\\nfor pred, similar_preds in predicate_similar.items():\\n    if pred not in model.wv:\\n        continue\\n    \\n    original_vector = model.wv[pred]\\n    \\n    for similar_pred in similar_preds:\\n        if similar_pred not in int_freq_by_pred:\\n            continue\\n        \\n        if similar_pred not in model.wv:\\n            continue\\n        \\n        for intensifier, frequency in int_freq_by_pred[similar_pred]:\\n            if frequency_threshold < frequency:\\n                continue\\n            \\n            combined_word = model.wv[similar_pred] + model.wv[intensifier]\\n            cosine_similarity = dot(original_vector, combined_word) /                 (norm(original_vector) * norm(combined_word))\\n            \\n            if cosine_similarity < pred_similarity_threshold:\\n                continue\\n        \\n            result.append(\\n                (pred, '%s %s' % (intensifier, similar_pred))\\n            )\\n\\nprint(result[:100])\\nlen(result)\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Appendix 2. Approach using word2vec\n",
    "# ====\n",
    "# I have approached with word2vec\n",
    "# Find words which has good similarity with \"very\". They are intensifiers\n",
    "# Find predicates which fulfills this condition:\n",
    "#   * Its similar word's vector + intensifier vector is similar with original predicates'vector\n",
    "\n",
    "\"\"\"\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "using_corpus = gutenberg\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "import re\n",
    "\n",
    "word_regex = re.compile('[a-zA-Z]+')\n",
    "\n",
    "def find_similar(word):\n",
    "    synsets = wn.synsets(word)\n",
    "    if len(synsets) < 1:\n",
    "        return []\n",
    "\n",
    "    synonyms = set()\n",
    "    for synset in synsets:\n",
    "        if synset.pos() == 'n':\n",
    "            continue\n",
    "            \n",
    "        synonyms.update(synset.lemma_names())\n",
    "\n",
    "    return synonyms\n",
    "\n",
    "import nltk\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "adverbs = {}\n",
    "predicates = {}\n",
    "adv_pred_bigrams = {}\n",
    "\n",
    "sentences_portion = .3\n",
    "sentences = using_corpus.sents()\n",
    "sentences = sentences[:int(len(sentences) * sentences_portion)]\n",
    "sentences = [\n",
    "    [ word.lower() for word in words if word_regex.match(word) ] for words in sentences\n",
    "]\n",
    "\n",
    "for sent in tqdm(sentences):\n",
    "    tokens = nltk.pos_tag(sent)\n",
    "    for index, (word, pos) in enumerate(tokens):\n",
    "        if pos not in ('RB', 'RBR', 'RBS'):\n",
    "            continue\n",
    "        \n",
    "        if len(tokens) <= index + 1:\n",
    "            continue\n",
    "        \n",
    "        next_word, next_pos = tokens[index + 1]\n",
    "        \n",
    "        if next_pos not in ('VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS'):\n",
    "            continue\n",
    "        \n",
    "        if word not in adverbs:\n",
    "            adverbs[word] = 0\n",
    "        \n",
    "        adverbs[word] += 1\n",
    "        \n",
    "        if next_word not in predicates:\n",
    "            predicates[next_word] = 0\n",
    "        \n",
    "        predicates[next_word] += 1\n",
    "        \n",
    "        bigram = (word, next_word)\n",
    "        if bigram not in adv_pred_bigrams:\n",
    "            adv_pred_bigrams[bigram] = 0\n",
    "        \n",
    "        adv_pred_bigrams[bigram] += 1\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(\n",
    "    sentences, \n",
    "    workers=4,\n",
    "    size=50,\n",
    "    min_count=10,\n",
    "    window=10,\n",
    "    sample=1e-3\n",
    ")\n",
    "\n",
    "bigrams = {}\n",
    "intensifiers = {}\n",
    "\n",
    "distance_threshold = 0.3\n",
    "general_frequency_threshold = 20.0\n",
    "\n",
    "### Filtering by very-similarity ###\n",
    "for bigram, frequency in adv_pred_bigrams.items():\n",
    "    adverb, predicate = bigram\n",
    "    \n",
    "    if adverb not in model.wv:\n",
    "        continue\n",
    "    \n",
    "    if adverbs[adverb] / frequency <= general_frequency_threshold:\n",
    "        continue\n",
    "    \n",
    "    if model.wv.distance(adverb, 'very') > distance_threshold:\n",
    "        continue\n",
    "        \n",
    "    if adverb not in intensifiers:\n",
    "        intensifiers[adverb] = 0\n",
    "        \n",
    "    intensifiers[adverb] += frequency\n",
    "    bigrams[bigram] = frequency\n",
    "\n",
    "print(len(intensifiers), list(intensifiers.keys())[:50])\n",
    "\n",
    "predicate_similar = {}\n",
    "\n",
    "for predicate in predicates:\n",
    "    similars = find_similar(predicate)\n",
    "    similar_predicates = [\n",
    "        similar for similar in similars\n",
    "        if similar in predicates and similar != predicate\n",
    "    ]\n",
    "    \n",
    "    if len(similar_predicates) > 0:\n",
    "        predicate_similar[predicate] = similar_predicates\n",
    "\n",
    "print(predicate_similar)\n",
    "\n",
    "int_freq_by_pred = {}\n",
    "\n",
    "for bigram, frequency in bigrams.items():\n",
    "    intensifier, predicate = bigram\n",
    "    if predicate not in int_freq_by_pred:\n",
    "        int_freq_by_pred[predicate] = {}\n",
    "    \n",
    "    int_freq_by_pred[predicate][intensifier] = frequency\n",
    "\n",
    "for predicate, int_freq in int_freq_by_pred.items():\n",
    "    int_freq_by_pred[predicate] = sorted(\n",
    "        int_freq.items(),\n",
    "        key = lambda x: x[1]\n",
    "    )\n",
    "\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "pred_similarity_threshold = 0.7\n",
    "frequency_threshold = 20\n",
    "result = []\n",
    "\n",
    "for pred, similar_preds in predicate_similar.items():\n",
    "    if pred not in model.wv:\n",
    "        continue\n",
    "    \n",
    "    original_vector = model.wv[pred]\n",
    "    \n",
    "    for similar_pred in similar_preds:\n",
    "        if similar_pred not in int_freq_by_pred:\n",
    "            continue\n",
    "        \n",
    "        if similar_pred not in model.wv:\n",
    "            continue\n",
    "        \n",
    "        for intensifier, frequency in int_freq_by_pred[similar_pred]:\n",
    "            if frequency_threshold < frequency:\n",
    "                continue\n",
    "            \n",
    "            combined_word = model.wv[similar_pred] + model.wv[intensifier]\n",
    "            cosine_similarity = dot(original_vector, combined_word) / \\\n",
    "                (norm(original_vector) * norm(combined_word))\n",
    "            \n",
    "            if cosine_similarity < pred_similarity_threshold:\n",
    "                continue\n",
    "        \n",
    "            result.append(\n",
    "                (pred, '%s %s' % (intensifier, similar_pred))\n",
    "            )\n",
    "\n",
    "print(result[:100])\n",
    "len(result)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
