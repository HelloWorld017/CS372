{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import DictReader\n",
    "\n",
    "DATASET_TARGET = 'test'\n",
    "DATASET_USE_SAMPLE = False\n",
    "\n",
    "with open('gap-%s.tsv' % DATASET_TARGET, newline='', encoding='utf-8') as csvfile:\n",
    "    dataset_reader = DictReader(csvfile, delimiter='\\t')\n",
    "    dataset = list(dataset_reader)\n",
    "\n",
    "    if DATASET_USE_SAMPLE:\n",
    "        dataset = dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started CoreNLP Server Successfully\n"
     ]
    }
   ],
   "source": [
    "from nltk.parse.corenlp import CoreNLPServer, CoreNLPDependencyParser\n",
    "\n",
    "server = CoreNLPServer(\n",
    "    \"./corenlp/stanford-corenlp-4.0.0.jar\",\n",
    "    \"./corenlp/stanford-corenlp-4.0.0-models.jar\"\n",
    ")\n",
    "server.start()\n",
    "\n",
    "print(\"Started CoreNLP Server Successfully\")\n",
    "\n",
    "parser = CoreNLPDependencyParser(url=server.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e2153d34974de0a18f3432055d5214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Match leaves of the tree with index in data\n",
    "# using index of word_tokenize\n",
    "# Also, change nonterminals from \"word\" to \"word/index in words\"\n",
    "def label_words(sent, start_index_sent, graph, word_index):\n",
    "    node_values = sorted(\n",
    "        [value for value in graph.nodes.values() if value['word'] is not None],\n",
    "        key=lambda item: item['word']\n",
    "    )\n",
    "    leave_addresses = [node['address'] for node in node_values]\n",
    "    leaves = [node['word'] for node in node_values]\n",
    "    \n",
    "    word_boundaries = list(word_tokenizer.span_tokenize(sent))\n",
    "    words = [sent[start:end] for (start, end) in word_boundaries]\n",
    "    \n",
    "    word_boundaries, words = zip(*sorted(\n",
    "        zip(word_boundaries, words),\n",
    "        key=lambda item: item[1]\n",
    "    ))\n",
    "    \n",
    "    matcher = SequenceMatcher(None, leaves, words, autojunk=False)\n",
    "    \n",
    "    i = word_index\n",
    "    word_results = []\n",
    "    \n",
    "    for a_idx_start, b_idx_start, size in matcher.get_matching_blocks():\n",
    "        for j in range(size):\n",
    "            a_idx = a_idx_start + j\n",
    "            b_idx = b_idx_start + j\n",
    "\n",
    "            graph.nodes[leave_addresses[a_idx]]['word'] = \"%s/%d\" % (leaves[a_idx], i)\n",
    "\n",
    "            start, end = word_boundaries[b_idx]\n",
    "            word_results.append((\"%s/%d\" % (words[b_idx], i), start + start_index_sent, end + start_index_sent))\n",
    "            i += 1\n",
    "    \n",
    "    word_results = sorted(\n",
    "        word_results,\n",
    "        key=lambda item: item[1]\n",
    "    )\n",
    "    \n",
    "    return tree, word_results, word_index + len(leaves)\n",
    "\n",
    "\n",
    "# 274, \"Cheryl Cassidy\" to [\"Cheryl/39\", \"Cassidy/40\"]\n",
    "def find_location(location, name, words):\n",
    "    location_end = location + len(name)\n",
    "    results = []\n",
    "    \n",
    "    for (word, start, end) in words:\n",
    "        if start <= location < end:\n",
    "            results.append(word)\n",
    "            continue\n",
    "        \n",
    "        if start < location_end <= end:\n",
    "            results.append(word)\n",
    "            continue\n",
    "        \n",
    "        if location < start and end < location_end:\n",
    "            results.append(word)\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Tokenize sentences, parse dependency of the sentence\n",
    "# and save it to the data_preprocessed\n",
    "data_preprocessed = []\n",
    "sent_tokenizer = PunktSentenceTokenizer()\n",
    "word_tokenizer = TreebankWordTokenizer()\n",
    "for data in tqdm(dataset):\n",
    "    data_text = data['Text']\n",
    "    word_index = 0\n",
    "    sent_boundaries = sent_tokenizer.span_tokenize(data_text)\n",
    "    sent_trees = []\n",
    "    sent_words = []\n",
    "    \n",
    "    for (start, end) in sent_boundaries:\n",
    "        sent = data_text[start:end]\n",
    "        tree = None\n",
    "        try:\n",
    "            tree = parser.parse_one(word_tokenize(sent))\n",
    "\n",
    "        except:\n",
    "            # In case of timeout or no tree\n",
    "            continue\n",
    "        \n",
    "        # Label words with its index in order to distinguish a specific word used at a specific position\n",
    "        # For example, I like her and her friends, is labeld as\n",
    "        # I/1 like/2 her/3 and/4 her/5 friends/6\n",
    "        # so I can distinguish her/3 and her/5\n",
    "        tree, words, word_index = label_words(sent, start, tree, word_index)\n",
    "        sent_trees.append(tree)\n",
    "        sent_words.append(words)\n",
    "    \n",
    "    sent_words_flatten = sum(sent_words, [])\n",
    "    \n",
    "    # Find which word does Offset point.\n",
    "    # (ex: (her, 8) points her/3 as 8-th character of \"I like her and her friends\" points her/3)\n",
    "    pron_words = find_location(int(data['Pronoun-offset']), data['Pronoun'], sent_words_flatten)\n",
    "    a_words = find_location(int(data['A-offset']), data['A'], sent_words_flatten)\n",
    "    b_words = find_location(int(data['B-offset']), data['B'], sent_words_flatten)\n",
    "    sent_words_noidx = [[word for word, start, end in words] for words in sent_words]\n",
    "    data_preprocessed.append({\n",
    "        'trees': sent_trees,\n",
    "        'words': sent_words_noidx,\n",
    "        'pron': pron_words,\n",
    "        'A': a_words,\n",
    "        'B': b_words\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.corpus import names as names_corpus\n",
    "from random import random\n",
    "\n",
    "def predict_gender(name):\n",
    "    return name_gender_classifier.classify(\n",
    "        name_feature(name)\n",
    "    )\n",
    "\n",
    "# Generate a feature, which will be used in Naive bayesian classifier, from name\n",
    "def name_feature(name):\n",
    "    return {\n",
    "        'first': name[0],\n",
    "        'suffix1': name[-1],\n",
    "        'suffix2': name[-2]\n",
    "    }\n",
    "\n",
    "name_gender_dataset = sorted(\n",
    "    (\n",
    "        [(name, 'male') for name in names_corpus.words('male.txt')] +\n",
    "        [(name, 'female') for name in names_corpus.words('female.txt')]\n",
    "    ), key = lambda k: random()\n",
    ")\n",
    "feature_gender_dataset = [\n",
    "    (name_feature(name), gender) for name, gender in name_gender_dataset\n",
    "]\n",
    "\n",
    "name_gender_classifier = NaiveBayesClassifier.train(feature_gender_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find sentence which includes the word\n",
    "def find_sent_from_word(sent_words, word):\n",
    "    return [i for i, words in enumerate(sent_words) if word in words][0]\n",
    "\n",
    "\n",
    "# Find triplets, which includes the word, from dependency graph\n",
    "def query(querying, words):\n",
    "    queried_features = []\n",
    "    for feature in querying:\n",
    "        (word, word_pos), rel, (target, target_pos) = feature\n",
    "        if word in words or target in words:\n",
    "            queried_features.append(feature)\n",
    "    \n",
    "    return queried_features\n",
    "\n",
    "\n",
    "# Determine a pronoun refers to the target word.\n",
    "def is_same(sent_trees, sent_words, pron_words, target_words):\n",
    "    if len(pron_words) == 0 or len(target_words) == 0:\n",
    "        return False\n",
    "    \n",
    "    pron_sentence = 0\n",
    "    target_sentence = 0\n",
    "    \n",
    "    try:\n",
    "        pron_sentence = find_sent_from_word(sent_words, pron_words[0])\n",
    "        target_sentence = find_sent_from_word(sent_words, target_words[0])\n",
    "    \n",
    "    except:\n",
    "        tqdm.write(\"Cannot find sent with target from data... skipping!\")\n",
    "        return False\n",
    "    \n",
    "    querying = list(sent_trees[pron_sentence].triples()) + \\\n",
    "        list(sent_trees[target_sentence].triples())\n",
    "    \n",
    "    words = pron_words + target_words\n",
    "    features = query(querying, words)\n",
    "    \n",
    "    # Pronoun cannot refer to word which is not mentioned before\n",
    "    if pron_sentence < target_sentence:\n",
    "        return False\n",
    "    \n",
    "    # Predict by depdency when two words are in the same sentences\n",
    "    if pron_sentence == target_sentence:\n",
    "        for ((word, _), rel, (target, __)) in features:\n",
    "            if 'subj' in rel:\n",
    "                queried_features = query(querying, [word])\n",
    "                obj = []\n",
    "                for ((qw, wpos), qr, (qt, tpos)) in queried_features:\n",
    "                    # Objective & Subjective -> NOT SAME\n",
    "                    if 'obj' in qr and qt in words:\n",
    "                        return False\n",
    "                    \n",
    "                    # Complement & Subjective -> SAME\n",
    "                    if 'comp' in qr and qt in words:\n",
    "                        return True\n",
    "    \n",
    "    # Predict by gender\n",
    "    pron_words_norm = [word.split('/')[0].lower() for word in pron_words]\n",
    "    pron_gender = 'female' if ('her', 'she') in pron_words_norm else 'male'\n",
    "\n",
    "    if pron_gender != predict_gender(' '.join(target_words)):\n",
    "        return False\n",
    "    \n",
    "    # If subject of previous sentence of pronoun is target\n",
    "    if pron_sentence == target_sentence + 1:\n",
    "        for ((word, _), rel, (target, __)) in query(\n",
    "            list(sent_trees[target_sentence].triples()),\n",
    "            target_words\n",
    "        ):\n",
    "            if 'subj' in rel:\n",
    "                return True\n",
    "    \n",
    "    # When it is not sure\n",
    "    return None\n",
    "\n",
    "\n",
    "same_count = 0\n",
    "results = []\n",
    "for data in data_preprocessed:\n",
    "    sent_trees, sent_words, pron_words, a_words, b_words = (\n",
    "        data['trees'], data['words'], data['pron'], data['A'], data['B']\n",
    "    )\n",
    "    \n",
    "    a_same = is_same(sent_trees, sent_words, pron_words, a_words)\n",
    "    b_same = is_same(sent_trees, sent_words, pron_words, b_words)\n",
    "    \n",
    "    if a_same is None and b_same is not None:\n",
    "        results.append((not b_same, b_same))\n",
    "        continue\n",
    "    \n",
    "    if a_same is not None and b_same is None:\n",
    "        results.append((a_same, not a_same))\n",
    "        continue\n",
    "    \n",
    "    if a_same is None and b_same is None:\n",
    "        if len(a_words) == 0 or len(b_words) == 0 or len(pron_words) == 0:\n",
    "            results.append((True, True))\n",
    "            continue\n",
    "        \n",
    "        # When it was unable to determine the pronoun refers to another word,\n",
    "        # flag it as uncertain and return the near word.\n",
    "        pron_sentence = find_sent_from_word(sent_words, pron_words[0])\n",
    "        a_sentence = find_sent_from_word(sent_words, a_words[0])\n",
    "        b_sentence = find_sent_from_word(sent_words, b_words[0])\n",
    "        \n",
    "        a_dist = pron_sentence - a_sentence\n",
    "        b_dist = pron_sentence - b_sentence\n",
    "        \n",
    "        results.append((None, False, True) if a_dist > b_dist else (None, True, False))\n",
    "        continue\n",
    "    \n",
    "    results.append((a_same, b_same))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import writer as CSVWriter\n",
    "\n",
    "# Write the result\n",
    "with open ('./results.tsv', 'w', encoding='utf-8') as csvfile:\n",
    "    writer = CSVWriter(csvfile, delimiter='\\t')\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        confident = result[0]\n",
    "        if confident is None:\n",
    "            a_coref, b_coref = result[1:]\n",
    "        \n",
    "        else:\n",
    "            a_coref, b_coref = result\n",
    "        \n",
    "        writer.writerow((\n",
    "            dataset[i]['ID'],\n",
    "            a_coref,\n",
    "            b_coref\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e63a281c8784df1bc87c4853e0a6686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import unquote\n",
    "from wikipediaapi import Wikipedia\n",
    "\n",
    "wiki = Wikipedia('en')\n",
    "wiki_crawled = {}\n",
    "\n",
    "# Finds uncertain results and crawl it from wikipedia\n",
    "for i, result in enumerate(tqdm(results)):\n",
    "    confident = result[0]\n",
    "    if confident is not None:\n",
    "        continue\n",
    "    \n",
    "    wiki_url = unquote(dataset[i]['URL'].replace('http://en.wikipedia.org/wiki/', ''))\n",
    "    wiki_text = wiki.page(wiki_url).summary\n",
    "    wiki_crawled[i] = wiki_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import json\n",
    "with open('./wikidump.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(wiki_crawled))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60919bd90f9b4c519be3bd4cc688f804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1224.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find sent with target from data... skipping!\n",
      "Cannot find sent with target from data... skipping!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_with_wiki(sent_trees, sent_words, pron_words, target_words, wiki_text):\n",
    "    target_normalized = ' '.join([t_word.split('/')[0] for t_word in target_words])\n",
    "    \n",
    "    # If A or B occur in wikipedia,\n",
    "    if target_normalized not in wiki_text:\n",
    "        return None, False\n",
    "    \n",
    "    # Preprocess the summary text\n",
    "    target_idx = wiki_text.index(target_normalized)\n",
    "    word_index = -10000\n",
    "    wiki_boundaries = sent_tokenizer.span_tokenize(wiki_text)\n",
    "    wiki_trees = []\n",
    "    wiki_words = []\n",
    "    \n",
    "    for (start, end) in wiki_boundaries:\n",
    "        sent = wiki_text[start:end]\n",
    "        tree = None\n",
    "        try:\n",
    "            tree = parser.parse_one(word_tokenize(sent))\n",
    "\n",
    "        except:\n",
    "            # In case of timeout or no tree\n",
    "            continue\n",
    "        \n",
    "        tree, words, word_index = label_words(sent, start, tree, word_index)\n",
    "        wiki_trees.append(tree)\n",
    "        wiki_words.append(words)\n",
    "    \n",
    "    wiki_words_flatten = sum(wiki_words, [])\n",
    "    wiki_target_words = find_location(target_idx, target_normalized, wiki_words_flatten)\n",
    "    \n",
    "    # Combine the summary text with original text\n",
    "    total_sents = [[word for word, start, end in words] for words in wiki_words] + sent_words\n",
    "    total_trees = wiki_trees + sent_trees\n",
    "    pron_sentence = 0\n",
    "    target_sentence = 0\n",
    "    \n",
    "    # Find two words, pronoun and (A or B)\n",
    "    try:\n",
    "        pron_sentence = find_sent_from_word(total_sents, pron_words[0])\n",
    "        target_sentence = find_sent_from_word(total_sents, wiki_target_words[0])\n",
    "    \n",
    "    except:\n",
    "        tqdm.write(\"Cannot find sent with target from data... skipping!\")\n",
    "        return None, True\n",
    "    \n",
    "    # Remain only two sentences, which includes the pronoun or (A or B)\n",
    "    # Then, do the same steps\n",
    "    return is_same(\n",
    "        [total_trees[target_sentence], total_trees[pron_sentence]],\n",
    "        [total_sents[target_sentence], total_sents[pron_sentence]],\n",
    "        pron_words,\n",
    "        wiki_target_words\n",
    "    ), True\n",
    "\n",
    "\n",
    "for i, wiki_text in tqdm(wiki_crawled.items()):\n",
    "    data = data_preprocessed[i]\n",
    "    sent_trees, sent_words, pron_words, a_words, b_words = (\n",
    "        data['trees'], data['words'], data['pron'], data['A'], data['B']\n",
    "    )\n",
    "    \n",
    "    a_same, a_inwiki = predict_with_wiki(sent_trees, sent_words, pron_words, a_words, wiki_text)\n",
    "    b_same, b_inwiki = predict_with_wiki(sent_trees, sent_words, pron_words, a_words, wiki_text)\n",
    "        \n",
    "    # if a_same is None and b_same is None:\n",
    "    #     a_same = True if a_inwiki else None\n",
    "    #     b_same = True if b_inwiki else None\n",
    "    \n",
    "    if a_same is None and b_same is None:\n",
    "        continue\n",
    "\n",
    "    if a_same is None and b_same is not None:\n",
    "        results[i] = ((not b_same, b_same))\n",
    "        continue\n",
    "    \n",
    "    if a_same is not None and b_same is None:\n",
    "        results[i] = ((a_same, not a_same))\n",
    "        continue\n",
    "    \n",
    "    results[i] = ((a_same, b_same))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the result\n",
    "with open ('./results_page.tsv', 'w', encoding='utf-8') as csvfile:\n",
    "    writer = CSVWriter(csvfile, delimiter='\\t')\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        confident = result[0]\n",
    "        if confident is None:\n",
    "            a_coref, b_coref = result[1:]\n",
    "        \n",
    "        else:\n",
    "            a_coref, b_coref = result\n",
    "        \n",
    "        writer.writerow((\n",
    "            dataset[i]['ID'],\n",
    "            a_coref,\n",
    "            b_coref\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
